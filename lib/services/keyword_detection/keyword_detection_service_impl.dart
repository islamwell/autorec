import 'dart:async';
import 'dart:io';
import 'dart:math';
import 'package:flutter/foundation.dart';
import 'package:flutter_sound/flutter_sound.dart';
import 'package:path_provider/path_provider.dart';
import 'package:permission_handler/permission_handler.dart';
import '../../models/keyword_profile.dart';
import 'keyword_detection_service.dart';
import '../audio/audio_quality_analyzer.dart';

/// Implementation of KeywordDetectionService with pattern matching and speech detection
///
/// IMPORTANT IMPROVEMENTS (Latest Update):
/// =====================================
/// This implementation has been significantly improved to fix false positives:
///
/// 1. **Speech Detection Gate**: Pattern matching now ONLY runs when speech is detected.
///    - Uses AudioQualityAnalyzer to verify speech presence before matching
///    - Prevents false triggers from ambient noise, silence, or background sounds
///    - Requires sustained speech patterns (not just momentary noise spikes)
///
/// 2. **Increased Confidence Threshold**: Raised from 0.3 to 0.65 (30% ‚Üí 65%)
///    - Reduces sensitivity to prevent false matches
///    - Requires stronger pattern correlation for keyword detection
///
/// 3. **Improved Pattern Extraction**: Now uses actual file content instead of synthetic data
///    - Samples actual audio file bytes to create unique fingerprints
///    - Each keyword recording produces a distinct pattern
///    - Previous implementation generated identical patterns for similar-duration files
///
/// 4. **Non-Speech Buffer Management**: Automatically clears buffer after prolonged silence
///    - Prevents stale audio data from affecting future detections
///    - Resets state when no speech detected for extended period
///
/// NOTE: This is still a simplified pattern matching approach. For production-grade
/// keyword detection, consider implementing:
/// - MFCC (Mel-Frequency Cepstral Coefficients) feature extraction
/// - FFT-based frequency analysis
/// - ML models (TensorFlow Lite, etc.)
/// - Audio fingerprinting algorithms (like Chromaprint/AcoustID)
///
/// This implementation serves as a foundation that can be extended with ML later.
class KeywordDetectionServiceImpl implements KeywordDetectionService {
  FlutterSoundRecorder? _recorder;
  StreamController<bool>? _keywordDetectedController;
  StreamController<double>? _confidenceController;
  Timer? _listeningTimer;
  
  // Audio buffer management
  final List<double> _audioBuffer = [];
  static const int _bufferSizeMs = 3000; // 3 seconds buffer
  static const int _sampleRate = 16000; // 16kHz for voice
  static const int _maxBufferSize = _bufferSizeMs * _sampleRate ~/ 1000;
  
  // Detection state
  bool _isListening = false;
  bool _isBackgroundListening = false;
  KeywordProfile? _currentProfile;
  double _confidenceThreshold = 0.65; // Increased threshold to reduce false positives
  
  // Background listening configuration
  bool _lowPowerMode = false;
  Duration _maxBackgroundDuration = const Duration(hours: 8);
  Timer? _backgroundTimeoutTimer;
  
  // Simple pattern matching state
  List<double>? _keywordPattern;
  static const double _patternMatchThreshold = 0.8;

  // Counter for logging frequency
  int _checkCount = 0;

  // Audio quality analyzer for speech detection
  final AudioQualityAnalyzer _audioAnalyzer = AudioQualityAnalyzer();
  int _consecutiveNonSpeechChecks = 0;
  static const int _maxConsecutiveNonSpeech = 20; // 2 seconds of no speech before resetting

  @override
  Stream<bool> get keywordDetectedStream => 
      _keywordDetectedController?.stream ?? const Stream.empty();

  @override
  Stream<double> get confidenceStream =>
      _confidenceController?.stream ?? const Stream.empty();

  @override
  bool get isListening => _isListening;

  @override
  bool get isBackgroundListening => _isBackgroundListening;

  @override
  KeywordProfile? get currentProfile => _currentProfile;

  /// Initialize the keyword detection service
  Future<void> _initialize() async {
    _recorder = FlutterSoundRecorder();
    _keywordDetectedController = StreamController<bool>.broadcast();
    _confidenceController = StreamController<double>.broadcast();

    try {
      await _recorder!.openRecorder();
      
      // Configure for continuous listening with low latency
      await _recorder!.setSubscriptionDuration(
        const Duration(milliseconds: 50), // 50ms updates for responsive detection
      );
    } catch (e) {
      throw KeywordDetectionException(
        'Failed to initialize keyword detection: ${e.toString()}',
        e,
      );
    }
  }

  @override
  Future<KeywordProfile> trainKeyword(String audioPath) async {
    try {
      if (kDebugMode) debugPrint('üéì [KW-TRAIN] Starting keyword training...');
      if (kDebugMode) debugPrint('üéì [KW-TRAIN] Audio path: $audioPath');

      // Validate audio file exists
      final audioFile = File(audioPath);
      if (!await audioFile.exists()) {
        throw KeywordDetectionException('Audio file not found: $audioPath');
      }

      final fileSize = await audioFile.length();
      if (kDebugMode) debugPrint('üéì [KW-TRAIN] File size: ${fileSize} bytes');

      // Extract audio pattern from the training file
      final pattern = await _extractAudioPattern(audioPath);

      if (kDebugMode) {
        debugPrint('üéì [KW-TRAIN] Pattern extracted successfully');
        debugPrint('üéì [KW-TRAIN] Pattern length: ${pattern.length}');
        debugPrint('üéì [KW-TRAIN] Pattern sample: [${pattern.take(5).map((v) => v.toStringAsFixed(3)).join(', ')}...]');
      }

      // Generate unique ID for the profile
      final profileId = DateTime.now().millisecondsSinceEpoch.toString();

      // Create keyword profile
      final profile = KeywordProfile(
        id: profileId,
        keyword: 'trained_keyword_$profileId', // Will be updated with actual keyword text
        modelPath: audioPath,
        trainedAt: DateTime.now(),
        confidence: _confidenceThreshold,
      );

      // Store the pattern for matching
      _keywordPattern = pattern;
      _currentProfile = profile;

      if (kDebugMode) debugPrint('‚úÖ [KW-TRAIN] Keyword training complete!');

      return profile;
    } catch (e) {
      if (kDebugMode) debugPrint('‚ùå [KW-TRAIN] Training failed: $e');
      throw KeywordDetectionException(
        'Failed to train keyword: ${e.toString()}',
        e,
      );
    }
  }

  @override
  Future<void> startListening() async {
    if (_isListening) {
      return; // Already listening
    }

    try {
      // Check microphone permission
      final permission = await Permission.microphone.status;
      if (!permission.isGranted) {
        throw KeywordDetectionException('Microphone permission not granted');
      }

      // Initialize if needed
      if (_recorder == null) {
        await _initialize();
      }

      if (_currentProfile == null || _keywordPattern == null) {
        throw KeywordDetectionException('No keyword profile loaded. Train a keyword first.');
      }

      // Start continuous audio monitoring
      await _startContinuousListening();
      _isListening = true;

    } catch (e) {
      throw KeywordDetectionException(
        'Failed to start listening: ${e.toString()}',
        e,
      );
    }
  }

  @override
  Future<void> stopListening() async {
    if (!_isListening) {
      return; // Already stopped
    }

    try {
      _listeningTimer?.cancel();
      _listeningTimer = null;

      if (_recorder?.isRecording == true) {
        await _recorder!.stopRecorder();
      }

      _isListening = false;
      _audioBuffer.clear();
      _audioAnalyzer.reset();
      _consecutiveNonSpeechChecks = 0;

    } catch (e) {
      throw KeywordDetectionException(
        'Failed to stop listening: ${e.toString()}',
        e,
      );
    }
  }

  @override
  Future<void> loadProfile(KeywordProfile profile) async {
    try {
      if (!profile.isValid()) {
        throw KeywordDetectionException('Invalid keyword profile');
      }

      // Load the audio pattern from the profile's model path
      final pattern = await _extractAudioPattern(profile.modelPath);
      
      _keywordPattern = pattern;
      _currentProfile = profile;
      _confidenceThreshold = profile.confidence;

    } catch (e) {
      throw KeywordDetectionException(
        'Failed to load profile: ${e.toString()}',
        e,
      );
    }
  }

  @override
  Future<void> updateConfidenceThreshold(double threshold) async {
    if (threshold < 0.0 || threshold > 1.0) {
      throw ArgumentError('Confidence threshold must be between 0.0 and 1.0');
    }

    _confidenceThreshold = threshold;
    
    // Update current profile if loaded
    if (_currentProfile != null) {
      _currentProfile = _currentProfile!.updateConfidence(threshold);
    }
  }

  @override
  Future<void> startBackgroundListening() async {
    if (_isBackgroundListening) {
      return; // Already in background mode
    }

    try {
      // Start regular listening first
      if (!_isListening) {
        await startListening();
      }

      // Configure for background mode
      await _configureBackgroundMode();
      
      _isBackgroundListening = true;

      // Set up background timeout
      _backgroundTimeoutTimer = Timer(_maxBackgroundDuration, () async {
        await stopBackgroundListening();
      });

    } catch (e) {
      throw KeywordDetectionException(
        'Failed to start background listening: ${e.toString()}',
        e,
      );
    }
  }

  @override
  Future<void> stopBackgroundListening() async {
    if (!_isBackgroundListening) {
      return; // Not in background mode
    }

    try {
      _backgroundTimeoutTimer?.cancel();
      _backgroundTimeoutTimer = null;
      
      // Restore normal listening mode
      await _restoreNormalMode();
      
      _isBackgroundListening = false;

    } catch (e) {
      throw KeywordDetectionException(
        'Failed to stop background listening: ${e.toString()}',
        e,
      );
    }
  }

  @override
  Future<void> configurePowerSettings({
    required bool lowPowerMode,
    required Duration maxBackgroundDuration,
  }) async {
    _lowPowerMode = lowPowerMode;
    _maxBackgroundDuration = maxBackgroundDuration;

    // Apply power settings if currently in background mode
    if (_isBackgroundListening) {
      await _configureBackgroundMode();
    }
  }

  @override
  Future<void> dispose() async {
    await stopBackgroundListening();
    await stopListening();
    
    _backgroundTimeoutTimer?.cancel();
    
    await _keywordDetectedController?.close();
    await _confidenceController?.close();
    
    if (_recorder != null) {
      await _recorder!.closeRecorder();
      _recorder = null;
    }
    
    _keywordDetectedController = null;
    _confidenceController = null;
    _currentProfile = null;
    _keywordPattern = null;
    _audioBuffer.clear();
  }

  /// Start continuous audio monitoring for keyword detection
  Future<void> _startContinuousListening() async {
    try {
      if (kDebugMode) debugPrint('üé§ [KW-DETECT] Starting continuous listening...');

      // Create temporary file for continuous recording
      final tempDir = await getTemporaryDirectory();
      final tempPath = '${tempDir.path}/keyword_listening_${DateTime.now().millisecondsSinceEpoch}.wav';

      if (kDebugMode) debugPrint('üé§ [KW-DETECT] Temp file: $tempPath');
      if (kDebugMode) debugPrint('üé§ [KW-DETECT] Pattern length: ${_keywordPattern?.length ?? 0}');
      if (kDebugMode) debugPrint('üé§ [KW-DETECT] Confidence threshold: $_confidenceThreshold');

      // Start recording with voice-optimized settings
      await _recorder!.startRecorder(
        toFile: tempPath,
        codec: Codec.pcm16WAV,
        sampleRate: _sampleRate,
        numChannels: 1, // Mono for voice
      );

      if (kDebugMode) debugPrint('üé§ [KW-DETECT] Recorder started successfully');

      // Set up audio level monitoring for buffer management
      _recorder!.onProgress!.listen((event) {
        if (event.decibels != null) {
          _processAudioLevel(event.decibels!);
        }
      });

      if (kDebugMode) debugPrint('üé§ [KW-DETECT] Audio level monitoring active');

      // Set up periodic pattern matching
      _listeningTimer = Timer.periodic(
        const Duration(milliseconds: 100),
        (_) => _performPatternMatching(),
      );

      if (kDebugMode) debugPrint('üé§ [KW-DETECT] Pattern matching timer started (100ms intervals)');

    } catch (e) {
      if (kDebugMode) debugPrint('‚ùå [KW-DETECT] Failed to start continuous listening: $e');
      throw KeywordDetectionException(
        'Failed to start continuous listening: ${e.toString()}',
        e,
      );
    }
  }

  /// Process incoming audio level and manage buffer
  void _processAudioLevel(double decibels) {
    // Convert decibels to normalized amplitude (0.0 to 1.0)
    final normalizedLevel = _normalizeDecibels(decibels);

    // Add to circular buffer
    _audioBuffer.add(normalizedLevel);

    // Analyze audio quality for speech detection
    final audioQuality = _audioAnalyzer.analyzeLevel(normalizedLevel);

    // Track consecutive non-speech periods
    if (!audioQuality.isSpeechDetected) {
      _consecutiveNonSpeechChecks++;
    } else {
      _consecutiveNonSpeechChecks = 0;
    }

    // Log every 50th sample to avoid spam
    if (_audioBuffer.length % 50 == 0 && kDebugMode) {
      debugPrint('üé§ [KW-DETECT] Audio: dB=$decibels, normalized=$normalizedLevel, buffer=${_audioBuffer.length}, speech=${audioQuality.isSpeechDetected}');
    }

    // Maintain buffer size (sliding window)
    if (_audioBuffer.length > _maxBufferSize) {
      _audioBuffer.removeAt(0);
    }
  }

  /// Normalize decibel values to 0.0-1.0 range for pattern matching
  double _normalizeDecibels(double decibels) {
    // Typical voice range: -60dB (quiet) to -10dB (loud)
    const double minDb = -60.0;
    const double maxDb = -10.0;
    
    // Clamp and normalize
    final clampedDb = decibels.clamp(minDb, maxDb);
    return (clampedDb - minDb) / (maxDb - minDb);
  }

  /// Perform simple pattern matching against the trained keyword
  void _performPatternMatching() {
    if (_keywordPattern == null) {
      if (kDebugMode) debugPrint('‚ö†Ô∏è [KW-DETECT] No keyword pattern loaded!');
      return;
    }

    if (_audioBuffer.length < _keywordPattern!.length) {
      // Still building buffer, don't spam logs
      return;
    }

    // CRITICAL FIX: Check if speech is currently detected
    // This prevents false triggers from background noise
    final currentQuality = _audioAnalyzer.currentQuality;
    if (currentQuality == null || !currentQuality.isSpeechDetected) {
      // Reset confidence when no speech is detected
      _confidenceController?.add(0.0);

      // Log every 20 checks to avoid spam
      _checkCount++;
      if (_checkCount % 20 == 0 && kDebugMode) {
        debugPrint('üîá [KW-DETECT] No speech detected - skipping pattern matching');
      }
      return;
    }

    // Only proceed with pattern matching if speech is consistently detected
    if (_consecutiveNonSpeechChecks > _maxConsecutiveNonSpeech) {
      // Clear buffer if we had a long period of no speech
      _audioBuffer.clear();
      _consecutiveNonSpeechChecks = 0;
      return;
    }

    // Get the most recent audio segment matching keyword length
    final segmentLength = _keywordPattern!.length;
    final recentSegment = _audioBuffer.sublist(
      _audioBuffer.length - segmentLength,
      _audioBuffer.length,
    );

    // Calculate similarity using cross-correlation
    final confidence = _calculateSimilarity(recentSegment, _keywordPattern!);

    // Log confidence every 2 seconds (20 checks at 100ms intervals)
    _checkCount++;
    if (_checkCount % 20 == 0 && kDebugMode) {
      debugPrint('üéØ [KW-DETECT] Speech detected! Confidence: ${(confidence * 100).toStringAsFixed(1)}% (threshold: ${(_confidenceThreshold * 100).toStringAsFixed(1)}%)');
    }

    // Emit confidence level
    _confidenceController?.add(confidence);

    // Check if confidence exceeds threshold
    if (confidence >= _confidenceThreshold) {
      if (kDebugMode) {
        debugPrint('');
        debugPrint('üéâüéâüéâ [KW-DETECT] KEYWORD DETECTED! üéâüéâüéâ');
        debugPrint('üéØ [KW-DETECT] Confidence: ${(confidence * 100).toStringAsFixed(1)}%');
        debugPrint('üéØ [KW-DETECT] Threshold: ${(_confidenceThreshold * 100).toStringAsFixed(1)}%');
        debugPrint('üéØ [KW-DETECT] Audio Quality: ${currentQuality.quality}');
        debugPrint('üéØ [KW-DETECT] SNR: ${currentQuality.signalToNoiseRatio.toStringAsFixed(1)} dB');
        debugPrint('');
      }

      _keywordDetectedController?.add(true);

      // Add small delay to prevent multiple rapid detections
      Future.delayed(const Duration(milliseconds: 500), () {
        _keywordDetectedController?.add(false);
      });
    }
  }

  /// Calculate similarity between two audio patterns using multiple comparison methods
  /// Returns the maximum confidence from different comparison techniques
  double _calculateSimilarity(List<double> segment1, List<double> segment2) {
    if (segment1.length != segment2.length) {
      return 0.0;
    }

    // Use multiple comparison methods and take the maximum
    // This makes keyword detection more robust
    final correlationScore = _normalizedCrossCorrelation(segment1, segment2);
    final energyScore = _energyBasedSimilarity(segment1, segment2);
    final shapeScore = _shapeMatchingSimilarity(segment1, segment2);
    final dtwScore = _simpleDTWSimilarity(segment1, segment2);

    // Take weighted average of all scores
    final combinedScore = (
      correlationScore * 0.25 +
      energyScore * 0.25 +
      shapeScore * 0.25 +
      dtwScore * 0.25
    );

    if (kDebugMode && _checkCount % 100 == 0) {
      debugPrint('üìä [KW-DETECT] Scores: corr=${(correlationScore*100).toStringAsFixed(1)}% energy=${(energyScore*100).toStringAsFixed(1)}% shape=${(shapeScore*100).toStringAsFixed(1)}% dtw=${(dtwScore*100).toStringAsFixed(1)}% combined=${(combinedScore*100).toStringAsFixed(1)}%');
    }

    return combinedScore;
  }

  /// Normalized cross-correlation (original method)
  double _normalizedCrossCorrelation(List<double> segment1, List<double> segment2) {
    final mean1 = segment1.reduce((a, b) => a + b) / segment1.length;
    final mean2 = segment2.reduce((a, b) => a + b) / segment2.length;

    double numerator = 0.0;
    double denominator1 = 0.0;
    double denominator2 = 0.0;

    for (int i = 0; i < segment1.length; i++) {
      final diff1 = segment1[i] - mean1;
      final diff2 = segment2[i] - mean2;

      numerator += diff1 * diff2;
      denominator1 += diff1 * diff1;
      denominator2 += diff2 * diff2;
    }

    final denominator = sqrt(denominator1 * denominator2);
    if (denominator == 0.0) return 0.0;

    return (numerator / denominator).abs();
  }

  /// Energy-based similarity - compares overall energy patterns
  double _energyBasedSimilarity(List<double> segment1, List<double> segment2) {
    final energy1 = sqrt(segment1.map((v) => v * v).reduce((a, b) => a + b));
    final energy2 = sqrt(segment2.map((v) => v * v).reduce((a, b) => a + b));

    if (energy1 == 0.0 || energy2 == 0.0) return 0.0;

    // Similarity is higher when energies are similar
    final energyRatio = min(energy1, energy2) / max(energy1, energy2);

    // Also compare energy distribution
    final normalized1 = segment1.map((v) => v / energy1).toList();
    final normalized2 = segment2.map((v) => v / energy2).toList();

    double sumDiff = 0.0;
    for (int i = 0; i < normalized1.length; i++) {
      sumDiff += (normalized1[i] - normalized2[i]).abs();
    }

    final distributionScore = 1.0 - (sumDiff / normalized1.length).clamp(0.0, 1.0);

    return (energyRatio + distributionScore) / 2.0;
  }

  /// Shape matching - compares the overall shape/envelope of the patterns
  double _shapeMatchingSimilarity(List<double> segment1, List<double> segment2) {
    // Compare peaks, valleys, and general shape
    final peaks1 = _findPeaks(segment1);
    final peaks2 = _findPeaks(segment2);

    // If peak counts are vastly different, lower score
    final peakCountSimilarity = peaks1.length == 0 || peaks2.length == 0
        ? 0.5
        : min(peaks1.length, peaks2.length) / max(peaks1.length, peaks2.length);

    // Compare slope patterns
    double slopeScore = 0.0;
    for (int i = 1; i < segment1.length; i++) {
      final slope1 = segment1[i] - segment1[i - 1];
      final slope2 = segment2[i] - segment2[i - 1];

      // Reward similar slope directions
      if (slope1 > 0 && slope2 > 0 || slope1 < 0 && slope2 < 0) {
        slopeScore += 1.0;
      }
    }
    slopeScore /= (segment1.length - 1);

    return (peakCountSimilarity + slopeScore) / 2.0;
  }

  /// Simplified DTW (Dynamic Time Warping) - allows for slight time shifts
  double _simpleDTWSimilarity(List<double> segment1, List<double> segment2) {
    // For performance, use a simplified DTW with limited window
    const int windowSize = 5;

    double totalDistance = 0.0;
    int matchCount = 0;

    for (int i = 0; i < segment1.length; i++) {
      // Look for best match within a small window
      final start = max(0, i - windowSize);
      final end = min(segment2.length - 1, i + windowSize);

      double minDist = double.infinity;
      for (int j = start; j <= end; j++) {
        final dist = (segment1[i] - segment2[j]).abs();
        minDist = min(minDist, dist);
      }

      totalDistance += minDist;
      matchCount++;
    }

    final avgDistance = totalDistance / matchCount;

    // Convert distance to similarity (lower distance = higher similarity)
    return 1.0 - avgDistance.clamp(0.0, 1.0);
  }

  /// Find peaks in a signal
  List<int> _findPeaks(List<double> signal) {
    final peaks = <int>[];

    for (int i = 1; i < signal.length - 1; i++) {
      if (signal[i] > signal[i - 1] && signal[i] > signal[i + 1]) {
        // This is a peak
        if (signal[i] > 0.3) { // Only count significant peaks
          peaks.add(i);
        }
      }
    }

    return peaks;
  }

  /// Configure audio settings for background mode with power optimization
  Future<void> _configureBackgroundMode() async {
    try {
      if (_recorder == null) return;

      // Adjust settings for low power consumption in background
      if (_lowPowerMode) {
        // Reduce update frequency for power saving
        await _recorder!.setSubscriptionDuration(
          const Duration(milliseconds: 200), // Less frequent updates
        );
        
        // Reduce buffer size to save memory
        _audioBuffer.clear();
        
        // Adjust pattern matching frequency
        _listeningTimer?.cancel();
        _listeningTimer = Timer.periodic(
          const Duration(milliseconds: 500), // Less frequent pattern matching
          (_) => _performPatternMatching(),
        );
      } else {
        // Standard background mode settings
        await _recorder!.setSubscriptionDuration(
          const Duration(milliseconds: 100), // Moderate update frequency
        );
        
        _listeningTimer?.cancel();
        _listeningTimer = Timer.periodic(
          const Duration(milliseconds: 200),
          (_) => _performPatternMatching(),
        );
      }

    } catch (e) {
      throw KeywordDetectionException(
        'Failed to configure background mode: ${e.toString()}',
        e,
      );
    }
  }

  /// Restore normal listening mode settings
  Future<void> _restoreNormalMode() async {
    try {
      if (_recorder == null) return;

      // Restore normal update frequency
      await _recorder!.setSubscriptionDuration(
        const Duration(milliseconds: 50), // High frequency for responsive detection
      );
      
      // Restore normal pattern matching frequency
      _listeningTimer?.cancel();
      _listeningTimer = Timer.periodic(
        const Duration(milliseconds: 100),
        (_) => _performPatternMatching(),
      );

    } catch (e) {
      throw KeywordDetectionException(
        'Failed to restore normal mode: ${e.toString()}',
        e,
      );
    }
  }

  /// Extract audio pattern from training file (improved implementation)
  /// This creates a more unique fingerprint based on file content hash
  /// NOTE: This is still a simplified approach. For production, use MFCC features or ML models.
  Future<List<double>> _extractAudioPattern(String audioPath) async {
    try {
      final audioFile = File(audioPath);
      if (!await audioFile.exists()) {
        throw KeywordDetectionException('Audio file not found: $audioPath');
      }

      // Read actual file bytes to create unique pattern
      final fileBytes = await audioFile.readAsBytes();
      final fileSize = fileBytes.length;

      if (kDebugMode) {
        debugPrint('üìä [PATTERN] Extracting pattern from ${fileBytes.length} bytes');
      }

      // Estimate duration based on file size
      final estimatedDurationMs = _estimateAudioDuration(fileSize, audioPath);

      // Create pattern length proportional to duration
      // Typical keywords are 0.5-2 seconds
      final patternLength = (estimatedDurationMs / 10).clamp(50, 200).toInt();

      // IMPROVED: Generate pattern based on actual file content
      // This creates a more unique signature for each recording
      final pattern = <double>[];

      // Sample the file bytes to create acoustic fingerprint
      // We'll sample evenly across the file to capture temporal characteristics
      final sampleInterval = max(1, fileBytes.length ~/ patternLength);

      for (int i = 0; i < patternLength; i++) {
        // Get sample position in file
        final bytePos = min((i * sampleInterval), fileBytes.length - 4);

        // Read 4 bytes and combine them into a pattern value
        // This captures actual file content, making each pattern unique
        int value = 0;
        for (int j = 0; j < 4 && (bytePos + j) < fileBytes.length; j++) {
          value = (value << 8) | fileBytes[bytePos + j];
        }

        // Normalize to 0.0-1.0 range
        final normalized = (value.abs() % 1000) / 1000.0;

        // Apply temporal weighting to emphasize middle sections
        // Speech has characteristic attack-sustain-decay envelope
        final position = i / patternLength;
        double temporalWeight = 1.0;

        if (position < 0.1) {
          // Attack phase - rising weight
          temporalWeight = position / 0.1;
        } else if (position > 0.85) {
          // Decay phase - falling weight
          temporalWeight = (1.0 - position) / 0.15;
        }

        // Combine content-based value with temporal weighting
        final weightedValue = (normalized * 0.7 + temporalWeight * 0.3).clamp(0.0, 1.0);
        pattern.add(weightedValue);
      }

      if (kDebugMode) {
        // Calculate pattern statistics for debugging
        final avgValue = pattern.reduce((a, b) => a + b) / pattern.length;
        final maxValue = pattern.reduce((a, b) => a > b ? a : b);
        final minValue = pattern.reduce((a, b) => a < b ? a : b);
        debugPrint('üìä [PATTERN] Generated pattern: length=$patternLength, avg=${avgValue.toStringAsFixed(3)}, min=${minValue.toStringAsFixed(3)}, max=${maxValue.toStringAsFixed(3)}');
      }

      return pattern;
    } catch (e) {
      throw KeywordDetectionException(
        'Failed to extract audio pattern: ${e.toString()}',
        e,
      );
    }
  }

  /// Estimate audio duration from file size and extension
  int _estimateAudioDuration(int fileSize, String filePath) {
    final extension = filePath.toLowerCase().split('.').last;

    if (extension == 'wav') {
      // WAV is uncompressed: ~32KB per second for 16kHz mono
      return (fileSize / 32000 * 1000).toInt();
    } else if (extension == 'm4a' || extension == 'aac' || extension == 'mp4') {
      // AAC is compressed: ~2KB per second at 64kbps
      return (fileSize / 2000 * 1000).toInt();
    } else {
      // Default assumption
      return (fileSize / 8000 * 1000).toInt();
    }
  }
}